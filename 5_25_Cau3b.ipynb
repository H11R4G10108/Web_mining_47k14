{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import string\n",
    "import re\n",
    "import emot\n",
    "from underthesea import sent_tokenize\n",
    "from underthesea import word_tokenize\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from emot.emo_unicode import UNICODE_EMOJI \n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def remove_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "def converting_emojis(text):\n",
    "    for x in EMOTICONS_EMO:\n",
    "        text = text.replace(x, \"_\".join(EMOTICONS_EMO[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    for x in UNICODE_EMOJI:\n",
    "        text = text.replace(x, \"_\".join(UNICODE_EMOJI[x].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "filename=os.path.join('D:', 'vietnamese-stopwords.txt')    \n",
    "with open(filename, 'r',encoding='utf-8') as p:\n",
    "            List_StopWords=p.read().split(\"\\n\") \n",
    "directory = 'D:\\Data1'\n",
    "txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "str=''\n",
    "for file in txt_files:\n",
    "    filename=os.path.join('D:\\Data1', file)    \n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    str=str+','+text\n",
    "str=str.replace(\"\\n\",\"\")  # Remove the newstr command\n",
    "str=str.replace(\"_\",\" \")\n",
    "str=str.lower() # Convert text to lowercase\n",
    "str = re.sub(\"\\d+\", \" \", str) # Remove number\n",
    "str = re.sub(r\"[!_@#$[]()]'\", \"\", str) # Remove character: !@#$[]()\n",
    "str=remove_tags(str)\n",
    "str=converting_emojis(str)\n",
    "str= \" \".join(str for str in str.split() if str not in List_StopWords)\n",
    "str = sent_tokenize(str)\n",
    "for i in range (0, len(str)):\n",
    "    str[i]=re.sub(r'[^\\w\\s]','',str[i])\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "features = vectorizer.fit_transform(str)\n",
    "print(features.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
